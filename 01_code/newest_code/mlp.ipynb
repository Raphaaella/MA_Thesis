{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60eda9f-8072-4a7d-bef6-566e9022915d",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c11df7-5b33-4f7c-a2ec-1da2f107822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Combine Features\n",
    "# Use the domain index and chat index as features\n",
    "# Extract the features for the MLP (concatenate start_year and title embedding)\n",
    "X = urls_sample.apply(lambda row: [row['start_year']] + [row['chat_index']] + list(row['title_embedding']), axis=1).tolist()\n",
    "\n",
    "# Convert the list of lists into a numpy array for training the MLP\n",
    "X = np.array(X)\n",
    "\n",
    "# Display the shape of the input features\n",
    "print(X.shape)\n",
    "\n",
    "#X = urls_sample[['start_year', 'chat_idx']].values\n",
    "y = urls_sample['pc1'].values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71201d1d-7410-4e6e-8fea-60854f42f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train an MLP Model\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the MLP regressor\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Display the first few predictions\n",
    "print(\"Predictions vs Actual:\\n\", pd.DataFrame({'Predicted': y_pred[:5], 'Actual': y_test[:5]}))\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculate R² for the model\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'R² Score: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151b907b-ee9e-423d-b11e-2a1edf89d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Predictions vs. Actual Values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7, color='b')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Ideal line\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"MLP Model: Predictions vs Actual Values\")\n",
    "plt.show()\n",
    "\n",
    "# Residual Plot\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(residuals, kde=True, bins=30)\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting training loss curve if available\n",
    "if hasattr(mlp, 'loss_curve_'):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(mlp.loss_curve_, label='Training Loss')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.title(\"MLP Training Loss Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Loss curve not available.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
