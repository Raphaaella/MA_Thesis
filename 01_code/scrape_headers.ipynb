{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raphaela\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\dask\\dataframe\\__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from urllib.parse import urlparse\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raphaela\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\dask\\dataframe\\io\\parquet\\core.py:1243: FutureWarning: The fastparquet engine is deprecated and will be removed in a future release. Please install pyarrow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "chats = pd.read_parquet('../02_data/data_archive/raphaela/chats.parquet', engine='pyarrow')\n",
    "en_domain_stats = pd.read_parquet('../02_data/data_archive/raphaela/en_domain_stats.parquet', engine='pyarrow')\n",
    "chat_url_shares = np.load('../02_data/data_archive/raphaela/chat_url_shares.npz')\n",
    "urls = dd.read_parquet('../02_data/data_archive/raphaela/urls.parquet', engine = 'fastparquet') # , engine='pyarrow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = urls.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://odysee.com/@jermwarfare:2/The-Baileys:8\n",
      "https://drsambailey.com/its-elementary-my-dear-watson-unmasking-the-viral-paradigm/\n",
      "https://drsambailey.com/why-nobody-had-caught-or-got-covid-19/\n",
      "https://live.childrenshealthdefense.org/shows/good-morning-chd/qtpO5WHxFz\n",
      "https://jonrappoport.substack.com/p/the-millionaire-blogger-in-the-land\n"
     ]
    }
   ],
   "source": [
    "for url in urls[\"url\"].head():\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user identification\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get headers of a URL\n",
    "def get_headers(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=5) # , verify=False, get instead of head?\n",
    "        return response.headers  # Return headers\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through URLs and get headers\n",
    "url_headers = {}\n",
    "for url in urls[\"url\"]:\n",
    "    headers = get_headers(url)\n",
    "    if headers:\n",
    "        url_headers[url] = headers\n",
    "\n",
    "# Display the headers of the first few URLs\n",
    "for url, headers in list(url_headers.items())[:5]:  # Displaying first 5 for brevity\n",
    "    print(f\"URL: {url}\")\n",
    "    print(\"Headers:\")\n",
    "    for key, value in headers.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.selenium.dev/selenium/web/web-form.html\")\n",
    "title = driver.title\n",
    "driver.implicitly_wait(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headers_selenium(url):\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    headers = {}\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(1)  # Wait for page to load (adjust as needed)\n",
    "        title = driver.title\n",
    "        headers = {url: title}\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return headers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'https://example.com': 'Example Domain'}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "url = \"https://example.com\"\n",
    "headers = get_headers_selenium(url)\n",
    "print(headers)\n",
    "\n",
    "# Close the driver when done\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_sample = urls.sample(n=100000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2043925    https://www.indiatoday.in/india/story/wrestler...\n",
       "540366     https://www.nytimes.com/2019/08/15/sports/base...\n",
       "768493     https://bigota.d.miui.com/V13.0.3.0.SKGINXM/mi...\n",
       "136266                                    http://ptv.io/2Jrx\n",
       "312520                          https://youtu.be/j5fCqKbSC7M\n",
       "Name: url, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_sample[\"url\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through URLs and get headers\n",
    "url_headers = []\n",
    "for url in urls_sample[\"url\"]:\n",
    "    headers = get_headers_selenium(url)\n",
    "    if headers:\n",
    "        url_headers.append(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'https://odysee.com/@jermwarfare:2/The-Baileys:8': 'The-Baileys'},\n",
       " {'https://drsambailey.com/its-elementary-my-dear-watson-unmasking-the-viral-paradigm/': '‘It’s Elementary My Dear Watson’ – Unmasking The Viral Paradigm'},\n",
       " {'https://drsambailey.com/why-nobody-had-caught-or-got-covid-19/': 'Why Nobody “Had, Caught or Got” COVID-19'},\n",
       " {'https://live.childrenshealthdefense.org/shows/good-morning-chd/qtpO5WHxFz': 'Page not found | Childrens Health Defense'},\n",
       " {'https://jonrappoport.substack.com/p/the-millionaire-blogger-in-the-land': 'The Millionaire blogger, in the Land of Virology'},\n",
       " {'https://planetwavesfm.substack.com/p/charlatans-web': \"Charlatan's Web - by Eric F Coppolino\"},\n",
       " {'https://drsambailey.com/resources/videos/natural-health-remedies/can-soft-drinks-be-healthy/': 'Can Soft Drinks Be Healthy?'},\n",
       " {'https://drsambailey.com/resources/videos/interviews/jon-rappoport-make-the-criminals-squirm/': 'Jon Rappoport: Make The Criminals Squirm'},\n",
       " {'https://drsambailey.com/resources/videos/germ-theory/stefan-lanka-virus-its-time-to-go/': 'Stefan Lanka: “Virus, It’s Time To Go.” - Dr Sam Bailey'},\n",
       " {'https://drsambailey.com/the-lazy-lies-of-roger-watson/': 'The Lazy Lies of Roger Watson - Dr Sam Bailey'},\n",
       " {'https://drsambailey.com/resources/videos/viruses-unplugged/what-about-rabies/': 'What About Rabies? - Dr Sam Bailey'},\n",
       " {'https://jermwarfare.com/tnt/mark-bailey': 'Mark Bailey on settling the virus debate - Jerm Warfare'},\n",
       " {'https://drsambailey.com/resources/videos/viruses-unplugged/virus-debate/': 'Virus Debate - Dr Sam Bailey'},\n",
       " {'https://drsambailey.com/resources/videos/interviews/baileys-vs-spacebusters/': 'Baileys vs Spacebusters'},\n",
       " {'https://t.me/debunkingthenonsense/3': 'Telegram: Contact @debunkingthenonsense'},\n",
       " {'https://youtu.be/nPSViaPCGrM': 'YouTube'},\n",
       " {'https://drsambailey.com/resources/videos/censorship/new-zealands-greatest-doctor/': 'New Zealand’s Greatest Doctor'},\n",
       " {'https://drsambailey.com/resources/settling-the-virus-debate/': 'The “Settling The Virus Debate” Statement - Dr Sam Bailey'},\n",
       " {'https://drsambailey.com/covid-19/lab-leaks-and-other-legends/': 'Lab Leaks and other Legends - Dr Sam Bailey'},\n",
       " {'https://drsambailey.com/viruses/monkeypox-mythology/': 'Monkeypox Mythology'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through URLs and get headers with a progress bar\n",
    "url_headers = []\n",
    "for url in tqdm(urls_sample[\"url\"], desc=\"Fetching headers\"):\n",
    "    headers = get_headers_selenium(url)\n",
    "    if headers:\n",
    "        url_headers.append(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 Approach\n",
    "filtering title from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_sample = urls.sample(n=100000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(url):\n",
    "\n",
    "    if 'youtube' in url.lower():\n",
    "        return 'YouTube'\n",
    "\n",
    "    # Pattern to extract the title from the URL (assuming it's between slashes and hyphens)\n",
    "    match = re.search(r'/([^/]+)/?$', url)  # Extract last part of URL after '/'\n",
    "    if match:\n",
    "        # Replace hyphens or underscores with spaces and return the title\n",
    "        title = match.group(1).replace('-', ' ').replace('_', ' ')\n",
    "        return title\n",
    "    return None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:01<00:00, 97311.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize tqdm to monitor the progress of apply\n",
    "tqdm.pandas()\n",
    "\n",
    "# Apply the function to the 'url' column with a progress bar\n",
    "urls_sample['title'] = urls_sample['url'].progress_apply(extract_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               id                                                url  \\\n",
      "2043925  31404053  https://www.indiatoday.in/india/story/wrestler...   \n",
      "540366   55066318  https://www.nytimes.com/2019/08/15/sports/base...   \n",
      "768493   52148717  https://bigota.d.miui.com/V13.0.3.0.SKGINXM/mi...   \n",
      "136266    2233418                                 http://ptv.io/2Jrx   \n",
      "312520   17089736                       https://youtu.be/j5fCqKbSC7M   \n",
      "...           ...                                                ...   \n",
      "668130   39465442      http://maps.google.com/?q=57.520810,25.331358   \n",
      "858521   30218649                      https://www.dixiememories.com   \n",
      "1018579   5212883                            https://t.co/PfXVDIT0qD   \n",
      "1680175   7971631  https://twitter.com/sovietvisuals/status/10091...   \n",
      "1829388  56355340  http://twitter.com/Darrmill/status/13606641053...   \n",
      "\n",
      "                        start_date                   end_date  \\\n",
      "2043925 2023-05-04 09:33:21.000000 2023-05-04 09:33:21.000000   \n",
      "540366  2019-08-16 06:19:03.000000 2019-08-16 06:19:03.000000   \n",
      "768493  2022-09-09 03:09:19.000000 2022-09-09 03:09:19.000000   \n",
      "136266  2023-06-08 00:12:11.574968 2023-06-17 01:11:31.037103   \n",
      "312520  2021-07-18 00:56:24.000000 2023-07-25 16:10:55.579514   \n",
      "...                            ...                        ...   \n",
      "668130  2023-11-12 17:41:18.000000 2023-11-12 17:41:18.000000   \n",
      "858521  2023-01-26 05:06:19.000000 2023-01-26 05:06:19.000000   \n",
      "1018579 2023-06-27 09:47:35.710601 2023-06-27 09:47:35.710601   \n",
      "1680175 2022-12-10 20:28:40.000000 2022-12-10 20:28:40.000000   \n",
      "1829388 2021-02-13 20:09:10.000000 2021-02-13 20:09:10.000000   \n",
      "\n",
      "                                                     title  \n",
      "2043925  wrestlers protest vinesh phogat emotional prot...  \n",
      "540366                mets braves.html?emc=rss&partner=rss  \n",
      "768493   miui blockota mojito in global V13.0.2.0.SKGIN...  \n",
      "136266                                                2Jrx  \n",
      "312520                                         j5fCqKbSC7M  \n",
      "...                                                    ...  \n",
      "668130                              ?q=57.520810,25.331358  \n",
      "858521                               www.dixiememories.com  \n",
      "1018579                                         PfXVDIT0qD  \n",
      "1680175                                1009167985167294464  \n",
      "1829388                                1360664105363849223  \n",
      "\n",
      "[100000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display the DataFrame with titles\n",
    "print(urls_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter non sense titles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Raphaela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "nltk.download('words')\n",
    "\n",
    "english_words = set(words.words()) - {\"a\"}  # Exclude \"a\" from the set of English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_valid_word(title):\n",
    "    # Check if title is not None\n",
    "    if title is None:\n",
    "        return False\n",
    "    # Split the title into words and check if any are in the list of English words\n",
    "    title_words = title.lower().split()\n",
    "    return any(word in english_words for word in title_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 31926 entries, 2043925 to 1086530\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   id          31926 non-null  int64         \n",
      " 1   url         31926 non-null  object        \n",
      " 2   start_date  31926 non-null  datetime64[ns]\n",
      " 3   end_date    31926 non-null  datetime64[ns]\n",
      " 4   title       31926 non-null  object        \n",
      "dtypes: datetime64[ns](2), int64(1), object(2)\n",
      "memory usage: 1.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataframe to include only titles with actual words\n",
    "filtered_df = urls_sample[urls_sample['title'].apply(contains_valid_word)]\n",
    "\n",
    "print(filtered_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save the DataFrame as a CSV file in the '02_data' folder\n",
    "folder_path = \"C:/Users/Raphaela/Documents/MA_Studium/4_Semester/MA_Thesis/02_data\"\n",
    "csv_path = os.path.join(folder_path, 'url_sample_with_titles.csv')\n",
    "filtered_df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress and save the DataFrame as a .gz file\n",
    "csv_gz_path = os.path.join(folder_path, 'url_sample_with_titles.csv.gz')\n",
    "filtered_df.to_csv(csv_gz_path, index=False, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
